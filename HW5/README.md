# Predicting Lorenz System Behavior with Feed Forward, Long Short Term Memory, Recurrent, and Echo State Networks

**Author**:

Qiyue Chen

**Abstract**:

In this study, we investigate the application of various neural network architectures, including feed-forward neural networks, Long Short-Term Memory (LSTM), Recurrent Neural Networks (RNN), and Echo State Networks (ESN), to predict future states of the Lorenz system.


---

## Introduction

This report aims to investigate the effectiveness and performance of various neural network architectures in predicting future states of the Lorenz system. The Lorenz system consists of three chaotic differential equations known for their intricate behavior.

The study involves training feed-forward neural networks, Long Short-Term Memory (LSTM) networks, Recurrent Neural Networks (RNNs), and Echo State Networks (ESNs) to advance the system's solution from time t to t + ∆t for specific values of the Lorenz system parameter ρ (ρ = 10, 28, and 40). The trained networks will then be evaluated on their ability to predict future states for different values of ρ (ρ = 17 and ρ = 35). This evaluation will assess the networks' generalization capability and accuracy in forecasting the dynamics of the Lorenz system under varying conditions.

By training and comparing the performances of these networks on the Lorenz system data, particularly in terms of their predictive accuracy for future states, we aim to gain insights into the strengths and limitations of each neural network architecture when handling complex and nonlinear time series data, such as those generated by the Lorenz system. This understanding will guide the selection and development of neural network architectures for similar tasks in the future.

---

## Theoretical Background

### The Lorenz System:
The Lorenz System comprises three differential equations initially formulated by Edward Lorenz in 1963 to represent simplified atmospheric convection patterns. The system exhibits chaotic behavior and is highly sensitive to initial conditions, often referred to as the "butterfly effect." The equations are as follows:

dx/dt = σ(y - x)

dy/dt = x(ρ - z) - y

dz/dt = xy - βz

Here, x, y, and z represent the system state, t denotes time, and σ, ρ, and β are the system parameters.

### Neural Networks (NN):
Neural Networks are machine learning models inspired by the structure and functioning of the human brain. They consist of interconnected layers of nodes or "neurons" that can learn patterns and make predictions based on input data. Neural Networks excel in processing complex, high-dimensional data, such as images or text, and can capture nonlinear relationships between inputs and outputs.

### Feedforward Neural Networks (FNNs):
Feedforward Neural Networks are a type of neural network where connections between nodes form a directed acyclic graph. In FNNs, information flows unidirectionally, moving from the input layer through the hidden layers to the output layer. This is in contrast to recurrent neural networks, which incorporate feedback connections, allowing information to loop through the network.

### Long Short-Term Memory (LSTM):
LSTM is a variant of recurrent neural networks (RNNs) specifically designed to capture long-term dependencies in sequential data. Traditional RNNs often face challenges such as vanishing or exploding gradients, which hinder their ability to capture long-range relationships. LSTMs overcome these limitations by employing memory cells and gating mechanisms, enabling effective modeling of long-term dependencies.

### Recurrent Neural Networks (RNNs):
RNNs are a type of neural network architecture designed to analyze and recognize patterns within sequences of data, such as text, genomes, or time series data. Unlike feedforward neural networks, RNNs possess internal memory, allowing them to process sequential inputs by leveraging their contextual information.

### Echo State Networks (ESN):
ESNs are a specific type of recurrent neural network featuring a sparsely connected hidden layer referred to as the reservoir. In ESNs, the weights of the connections between the input-hidden and hidden-hidden layers are randomly initialized and remain fixed. Only the weights from the hidden layer to the output layer are trained, simplifying and expediting the training process

In this assignment, we will utilize these diverse neural network architectures to predict future states of the Lorenz system for different values of the parameter ρ. The performance of these networks will be assessed based on their accuracy in forecasting the system dynamics.
---

## Algorithm Implementation and Development

Importing necessary libraries:

```
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams
from scipy import integrate
from mpl_toolkits.mplot3d import Axes3D
import torch
import torch.nn as nn
import torch.optim as optim
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import SimpleRNN
import tensorflow_addons as tfa
import tensorflow as tf
```

Generating Lorenz system data:
``` python
def lorenz_deriv(x_y_z, t0, sigma=10, beta=8/3, rho=28):
    x, y, z = x_y_z
    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]


def generate_data(rho, seed=123):
    dt = 0.01
    T = 8
    t = np.arange(0, T + dt, dt)

    np.random.seed(seed)
    x0 = -15 + 30 * np.random.random((100, 3))

    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(10, 8/3, rho)) for x0_j in x0])

    nn_input = np.zeros((100 * (len(t) - 1), 3))
    nn_output = np.zeros_like(nn_input)

    for j in range(100):
        nn_input[j * (len(t) - 1):(j + 1) * (len(t) - 1), :] = x_t[j, :-1, :]
        nn_output[j * (len(t) - 1):(j + 1) * (len(t) - 1), :] = x_t[j, 1:, :]

    return nn_input, nn_output
```

### Functions for creating and training models



#### Feed Forward Neural Network (FFNN)
```
def build_ffnn(input_shape):
    model = Sequential()
    model.add(Dense(64, activation='relu', input_shape=input_shape))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(input_shape[0], activation='linear')) # Output layer
    model.compile(optimizer='adam', loss='mse')
    return model

def train_ffnn(model, inputs, targets, epochs=100):
    model.fit(inputs, targets, epochs=epochs, verbose=1)
```

#### Long Short Term Memory Network (LSTM)
```
def build_lstm(input_shape):
    model = Sequential()
    model.add(LSTM(64, activation='relu', input_shape=input_shape))
    model.add(Dense(input_shape[1], activation='linear')) # Output layer
    model.compile(optimizer='adam', loss='mse')
    return model

def train_lstm(model, inputs, targets, epochs=100):
    model.fit(inputs, targets, epochs=epochs, verbose=1)
```

#### Recurrent Neural Network (RNN)
```
def build_rnn(input_shape):
    model = Sequential()
    model.add(SimpleRNN(64, activation='relu', input_shape=input_shape))
    model.add(Dense(input_shape[1], activation='linear')) # Output layer
    model.compile(optimizer='adam', loss='mse')
    return model

def train_rnn(model, inputs, targets, epochs=100):
    model.fit(inputs, targets, epochs=epochs, verbose=1)
```

#### Echo State Network (ESN)
```
def create_esn(input_shape, units, connectivity=0.1, leaky=1, spectral_radius=0.9):
    inputs = tf.keras.Input(shape=input_shape)
    esn_outputs = tfa.layers.ESN(units, connectivity, leaky, spectral_radius)(inputs)
    output = tf.keras.layers.Dense(3)(esn_outputs)
    
    model = tf.keras.Model(inputs=inputs, outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')
    return model

def train_esn(X_train, y_train, X_test, y_test, input_shape, reservoir_size, epochs=50, batch_size=32):
    esn_model = create_esn(input_shape, reservoir_size)
    esn_history = esn_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=2)
    return esn_model, esn_history
```

### Training models

Preparing data to train models:
```
# Generate data for each rho value
inputs_10, targets_10 = generate_data(10)
inputs_28, targets_28 = generate_data(28)
inputs_40, targets_40 = generate_data(40)

# Concatenate the inputs and targets
inputs = np.concatenate([inputs_10, inputs_28, inputs_40])
targets = np.concatenate([targets_10, targets_28, targets_40])
```

#### Feed Forward Neural Network (FFNN)
```
# Build the model
ffnn = build_ffnn(inputs.shape[1:])

# Train the model
train_ffnn(ffnn, inputs, targets, epochs=25)
```

Reshape inputs for LSTM, RNN, and ESN [samples, time steps, features]
```
inputs = inputs.reshape((inputs.shape[0], 1, inputs.shape[1]))
```

#### Long Short Term Memory Network (LSTM)
```
# Build the model
lstm = build_lstm(inputs.shape[1:])

# Train the model
train_lstm(lstm, inputs, targets, epochs=25)
```

#### Recurrent Neural Network (RNN)
```
# Build the model
rnn = build_rnn(inputs.shape[1:])

# Train the model
train_rnn(rnn, inputs, targets, epochs=25)
```

#### Echo State Network (ESN)
```
# Split data into training and test sets
train_size = int(0.8 * len(inputs))
X_train, X_test = inputs[:train_size], inputs[train_size:]
y_train, y_test = targets[:train_size], targets[train_size:]

# Define ESN parameters
input_shape = inputs.shape[1:]
reservoir_size = 64
epochs = 25
batch_size = 32

# Train the ESN model
esn_model, esn_history = train_esn(X_train, y_train, X_test, y_test, input_shape, reservoir_size, epochs, batch_size)
```

### Predicting Lorenz System for rho = 17, 35

Defining function for plotting predictions:
``` python
def plot_predictions(new_targets, predictions, title):
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    ax.plot(new_targets[:, 0], new_targets[:, 1], new_targets[:, 2], 'r', label='Actual')
    ax.plot(predictions[:, 0], predictions[:, 1], predictions[:, 2], 'b', label='Predicted')
    ax.set_title(title)
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.legend()
    plt.show()
```

#### Predicting with FFNN
```
# Generate new data for rho=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Use the trained model to make predictions for the new data
predictions_17 = ffnn.predict(inputs_17)

mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'FFNN Mean Squared Error on data with rho={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_17, f'FFNN for rho={rho_17}')

# Generate new data for rho=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Use the trained model to make predictions for the new data
predictions_35 = ffnn.predict(inputs_35)

mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'FFNN Mean Squared Error on data with rho={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'FFNN for rho={rho_35}')
```

#### Predicting with LSTM
```
# Generate new data for rho=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Reshape new_inputs for LSTM [samples, time steps, features]
inputs_17 = inputs_17.reshape((inputs_17.shape[0], 1, inputs_17.shape[1]))

# Use the trained model to make predictions for the new data
predictions_17 = lstm.predict(inputs_17)

# Calculate and print the MSE
mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'LSTM Mean Squared Error on data with rho={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_17, f'LSTM for rho={rho_17}')

# Generate new data for rho=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Reshape new_inputs for LSTM [samples, time steps, features]
inputs_35 = inputs_35.reshape((inputs_35.shape[0], 1, inputs_35.shape[1]))

# Use the trained model to make predictions for the new data
predictions_35 = lstm.predict(inputs_35)

# Calculate and print the MSE
mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'LSTM Mean Squared Error on data with rho={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'LSTM for rho={rho_35}')
```

#### Predicting with RNN
```
# Generate new data for rho=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Reshape new_inputs for RNN [samples, time steps, features]
inputs_17 = inputs_17.reshape((inputs_17.shape[0], 1, inputs_17.shape[1]))

# Use the trained model to make predictions for the new data
predictions_17 = rnn.predict(inputs_17)

# Calculate and print the MSE
mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'RNN Mean Squared Error on new data with rho={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_17, f'RNN for rho={rho_17}')

# Generate new data for rho=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Reshape new_inputs for RNN [samples, time steps, features]
inputs_35 = inputs_35.reshape((inputs_35.shape[0], 1, inputs_35.shape[1]))

# Use the trained model to make predictions for the new data
predictions_35 = rnn.predict(inputs_35)

# Calculate and print the MSE
mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'RNN Mean Squared Error on new data with rho={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'RNN for rho={rho_35}')
```

#### Predicting with ESN
```
# Generate new data for rho=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Reshape new_inputs for ESN [samples, time steps, features]
inputs_17 = inputs_17.reshape((inputs_17.shape[0], 1, inputs_17.shape[1]))

# Use the trained model to make predictions for the new data
predictions_17 = esn_model.predict(inputs_17)

# Calculate and print the MSE
mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'ESN Mean Squared Error on new data with rho={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_17, f'ESN for rho={rho_17}')

# Generate new data for rho=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Reshape new_inputs for ESN [samples, time steps, features]
inputs_35 = inputs_35.reshape((inputs_35.shape[0], 1, inputs_35.shape[1]))

# Use the trained model to make predictions for the new data
predictions_35 = esn_model.predict(inputs_35)

# Calculate and print the MSE
mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'ESN Mean Squared Error on new data with rho={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'ESN for rho={rho_35}')
```

---

## Computational Results

### Error on Lorenz System with Unseen Values of ρ

The table below shows the error rates (loss) obtained for each neural network architecture when predicting the Lorenz system's future states for unseen values of ρ.

| Neural Network | ρ = 17 | ρ = 35 |
|----------------|--------|--------|
| NN             | 51.02  | 119.07 |
| LSTM           | 34.12  | 55.88  |
| RNN            | 43.22  | 55.39  |
| ESN            | 20.92  | 38.05  |
Epochs: 50
---

## Summary and Conclusions

In this report, we first investigated the performance of various neural network architectures, including Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), Feed-Forward Neural Networks (FFNN), and Echo State Networks (ESN), in predicting future states of the Lorenz system for unseen values of the parameter ρ.

We trained each of these networks to advance the solution from time t to t + ∆t for specific values of ρ (ρ = 10, 28, and 40) and then evaluated their performance in predicting future states for ρ = 17 and ρ = 35. The performance of the networks was assessed based on the mean squared error (MSE) of their predictions.

For ρ = 17, the RNN model achieved the lowest MSE (0.006688), followed by the LSTM model (0.007858), the FFNN model (0.009169), and the ESN model (7.348020). For ρ = 35, the LSTM model achieved the lowest MSE (0.012208), followed by the RNN model (0.014535), the FFNN model (0.018447), and the ESN model (9.366474).

From these results, we can conclude that the RNN and LSTM models outperformed the FFNN and ESN models in predicting future states of the Lorenz system for unseen values of ρ. The ESN model, in particular, performed significantly worse than the other models, suggesting that it may not be well-suited to this task.

These findings provide valuable insights into the strengths and weaknesses of different neural network architectures in handling complex, non-linear time series data, such as those generated by the Lorenz system. They highlight the importance of selecting the appropriate architecture for the task at hand and provide a basis for further investigation into the factors that contribute to the superior performance of RNN and LSTM models in this context.