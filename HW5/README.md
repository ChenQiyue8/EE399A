# Prediction of Lorenz System Behavior using Neural Networks

**Author**:
Qiyue Chen

**Abstract**:
This report explores the application of different neural network architectures, including feed-forward neural networks, Long Short-Term Memory (LSTM) networks, Recurrent Neural Networks (RNNs), and Echo State Networks (ESNs), for predicting the future states of the Lorenz system.

## Introduction

The aim of this study is to assess the effectiveness and performance of various neural network architectures in predicting the future states of the Lorenz system. The Lorenz system is comprised of three chaotic differential equations known for their complex behavior.

The research involves training feed-forward neural networks, LSTM networks, RNNs, and ESNs to advance the system's solution from time t to t + ∆t for specific values of the Lorenz system parameter ρ (ρ = 10, 28, and 40). The trained networks will then be evaluated on their ability to predict future states for different values of ρ (ρ = 17 and ρ = 35). This evaluation will assess the networks' ability to generalize and accurately forecast the dynamics of the Lorenz system under varying conditions.

By training and comparing the performances of these networks on the Lorenz system data, particularly in terms of their predictive accuracy for future states, we aim to gain insights into the strengths and limitations of each neural network architecture when dealing with complex and nonlinear time series data, such as those generated by the Lorenz system. This understanding will guide the selection and development of neural network architectures for similar tasks in the future.

---

## Algorithm Implementation and Development

**Importing necessary libraries:**

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import integrate
from mpl_toolkits.mplot3d import Axes3D
import torch
import torch.nn as nn
import torch.optim as optim
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import SimpleRNN
import tensorflow_addons as tfa
import tensorflow as tf
```

**Generating Lorenz system data:**

```python
def lorenz_deriv(x_y_z, t0, sigma=10, beta=8/3, rho=28):
    x, y, z = x_y_z
    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]


def generate_data(rho, seed=123):
    dt = 0.01
    T = 8
    t = np.arange(0, T + dt, dt)

    np.random.seed(seed)
    x0 = -15 + 30 * np.random.random((100, 3))

    x_t = np.asarray([integrate.odeint(lorenz_deriv, x0_j, t, args=(10, 8/3, rho)) for x0_j in x0])

    nn_input = np.zeros((100 * (len(t) - 1), 3))
    nn_output = np.zeros_like(nn_input)

    for j in range(100):
        nn_input[j * (len(t) - 1):(j + 1) * (len(t) - 1), :] = x_t[j, :-1, :]
        nn_output[j * (len(t) - 1):(j + 1) * (len(t) - 1), :] = x_t[j, 1:, :]

    return nn_input, nn_output
```

**Functions for creating and training models**

**Feed Forward Neural Network (FFNN

):**

```python
def build_ffnn(input_shape):
    model = Sequential()
    model.add(Dense(64, activation='relu', input_shape=input_shape))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(input_shape[0], activation='linear')) # Output layer
    model.compile(optimizer='adam', loss='mse')
    return model

def train_ffnn(model, inputs, targets, epochs=100):
    model.fit(inputs, targets, epochs=epochs, verbose=1)
```

**Long Short Term Memory Network (LSTM):**

```python
def build_lstm(input_shape):
    model = Sequential()
    model.add(LSTM(64, activation='relu', input_shape=input_shape))
    model.add(Dense(input_shape[1], activation='linear')) # Output layer
    model.compile(optimizer='adam', loss='mse')
    return model

def train_lstm(model, inputs, targets, epochs=100):
    model.fit(inputs, targets, epochs=epochs, verbose=1)
```

**Recurrent Neural Network (RNN):**

```python
def build_rnn(input_shape):
    model = Sequential()
    model.add(SimpleRNN(64, activation='relu', input_shape=input_shape))
    model.add(Dense(input_shape[1], activation='linear')) # Output layer
    model.compile(optimizer='adam', loss='mse')
    return model

def train_rnn(model, inputs, targets, epochs=100):
    model.fit(inputs, targets, epochs=epochs, verbose=1)
```

**Echo State Network (ESN):**

```python
def create_esn(input_shape, units, connectivity=0.1, leaky=1, spectral_radius=0.9):
    inputs = tf.keras.Input(shape=input_shape)
    esn_outputs = tfa.layers.ESN(units, connectivity, leaky, spectral_radius)(inputs)
    output = tf.keras.layers.Dense(3)(esn_outputs)
    
    model = tf.keras.Model(inputs=inputs, outputs=output)
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')
    return model

def train_esn(X_train, y_train, X_test, y_test, input_shape, reservoir_size, epochs=50, batch_size=32):
    esn_model = create_esn(input_shape, reservoir_size)
    esn_history = esn_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=2)
    return esn_model, esn_history
```

**Training models**

**Preparing data to train models:**

```python
# Generate data for each rho value
inputs_10, targets_10 = generate_data(10)
inputs_28, targets_28 = generate_data(28)
inputs_40, targets_40 = generate_data(40)

# Concatenate the inputs and targets
inputs = np.concatenate([inputs_10, inputs_28, inputs_40])
targets = np.concatenate([targets_10, targets_28, targets_40])
```

**Feed Forward Neural Network (FFNN):**

```python
# Build the model
ffnn = build_ffnn(inputs.shape[1:])

# Train the model
train_ffnn(ffnn, inputs, targets, epochs=25)
```

**Reshape inputs for LSTM, RNN, and ESN [samples, time steps, features]:**

```python
inputs = inputs.reshape((inputs.shape[0], 1, inputs.shape[1]))
```

**Long Short Term Memory Network (LSTM):**

```python
# Build the model
lstm = build_lstm(inputs.shape[1:])

#

 Train the model
train_lstm(lstm, inputs, targets, epochs=25)
```

**Recurrent Neural Network (RNN):**

```python
# Build the model
rnn = build_rnn(inputs.shape[1:])

# Train the model
train_rnn(rnn, inputs, targets, epochs=25)
```

**Echo State Network (ESN):**

```python
# Split data into training and test sets
train_size = int(0.8 * len(inputs))
X_train, X_test = inputs[:train_size], inputs[train_size:]
y_train, y_test = targets[:train_size], targets[train_size:]

# Define ESN parameters
input_shape = inputs.shape[1:]
reservoir_size = 64
epochs = 25
batch_size = 32

# Train the ESN model
esn_model, esn_history = train_esn(X_train, y_train, X_test, y_test, input_shape, reservoir_size, epochs, batch_size)
```

**Predicting Lorenz System for ρ = 17, 35**

**Defining function for plotting predictions:**

```python
def plot_predictions(new_targets, predictions, title):
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')
    ax.plot(new_targets[:, 0], new_targets[:, 1], new_targets[:, 2], 'r', label='Actual')
    ax.plot(predictions[:, 0], predictions[:, 1], predictions[:, 2], 'b', label='Predicted')
    ax.set_title(title)
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.legend()
    plt.show()
```

**Predicting with FFNN:**

```python
# Generate new data for ρ=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Use the trained model to make predictions for the new data
predictions_17 = ffnn.predict(inputs_17)

mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'FFNN Mean Squared Error on data with ρ={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_17, f'FFNN for ρ={rho_17}')

# Generate new data for ρ=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Use the trained model to make predictions for the new data
predictions_35 = ffnn.predict(inputs_35)

mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'FFNN Mean Squared Error on data with ρ={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'FFNN for ρ={rho_35}')
```

**Predicting with LSTM:**

```python
# Generate new data for ρ=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Reshape new_inputs for LSTM [samples, time steps, features]
inputs_17 = inputs_17.reshape((inputs_17.shape[0], 1, inputs_17.shape[1]))

# Use the trained model to make predictions for the new data
predictions_17 = lstm.predict(inputs_17)

# Calculate and print the MSE
mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'LSTM Mean Squared Error on data with ρ={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_

17, f'LSTM for ρ={rho_17}')

# Generate new data for ρ=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Reshape new_inputs for LSTM [samples, time steps, features]
inputs_35 = inputs_35.reshape((inputs_35.shape[0], 1, inputs_35.shape[1]))

# Use the trained model to make predictions for the new data
predictions_35 = lstm.predict(inputs_35)

# Calculate and print the MSE
mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'LSTM Mean Squared Error on data with ρ={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'LSTM for ρ={rho_35}')
```

**Predicting with RNN:**

```python
# Generate new data for ρ=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Reshape new_inputs for RNN [samples, time steps, features]
inputs_17 = inputs_17.reshape((inputs_17.shape[0], 1, inputs_17.shape[1]))

# Use the trained model to make predictions for the new data
predictions_17 = rnn.predict(inputs_17)

# Calculate and print the MSE
mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'RNN Mean Squared Error on new data with ρ={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_17, f'RNN for ρ={rho_17}')

# Generate new data for ρ=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Reshape new_inputs for RNN [samples, time steps, features]
inputs_35 = inputs_35.reshape((inputs_35.shape[0], 1, inputs_35.shape[1]))

# Use the trained model to make predictions for the new data
predictions_35 = rnn.predict(inputs_35)

# Calculate and print the MSE
mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'RNN Mean Squared Error on new data with ρ={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'RNN for ρ={rho_35}')
```

**Predicting with ESN:**

```python
# Generate new data for ρ=17
rho_17 = 17
inputs_17, targets_17 = generate_data(rho_17)

# Reshape new_inputs for ESN [samples, time steps, features]
inputs_17 = inputs_17.reshape((inputs_17.shape[0], 1, inputs_17.shape[1]))

# Use the trained model to make predictions for the new data
predictions_17 = esn_model.predict(inputs_17)

# Calculate and print the MSE
mse_17 = mean_squared_error(targets_17, predictions_17)
print(f'ESN Mean Squared Error on new data with ρ={rho_17}: {mse_17}')

plot_predictions(targets_17, predictions_17, f'ESN for ρ={rho_17}')

# Generate new data for ρ=35
rho_35 = 35
inputs_35, targets_35 = generate_data(rho_35)

# Reshape new_inputs for ESN [samples, time steps, features]
inputs_35 = inputs_35.reshape((inputs_35.shape[0], 1, inputs_35.shape

[1]))

# Use the trained model to make predictions for the new data
predictions_35 = esn_model.predict(inputs_35)

# Calculate and print the MSE
mse_35 = mean_squared_error(targets_35, predictions_35)
print(f'ESN Mean Squared Error on new data with ρ={rho_35}: {mse_35}')

plot_predictions(targets_35, predictions_35, f'ESN for ρ={rho_35}')
```

---

## Computational Results

### Error on Lorenz System with Unseen Values of ρ

The table below presents the error rates (loss) obtained for each neural network architecture when predicting the Lorenz system's future states for unseen values of ρ.

| Neural Network | ρ = 17 | ρ = 35 |
|----------------|--------|--------|
| FFNN           | 51.02  | 119.07 |
| LSTM           | 34.12  | 55.88  |
| RNN            | 43.22  | 55.39  |
| ESN            | 20.92  | 38.05  |

Epochs: 50

---

## Summary and Conclusions

This report investigated the performance of different neural network architectures, including Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM) networks, Feed-Forward Neural Networks (FFNN), and Echo State Networks (ESN), in predicting the future states of the Lorenz system for unseen values of the parameter ρ.

Each network was trained to advance the solution from time t to t + ∆t for specific values of ρ (ρ = 10, 28, and 40) and then evaluated on their ability to predict future states for ρ = 17 and ρ = 35. The mean squared error (MSE) was used as a measure of performance.

The results indicate that the RNN and LSTM models outperformed the FFNN and ESN models in predicting the future states of the Lorenz system for unseen values of ρ. The ESN model performed significantly worse than the other models, suggesting that it may not be suitable for this task.

These findings provide insights into the strengths and limitations of different neural network architectures when dealing with complex, nonlinear time series data such as the Lorenz system. They highlight the importance of selecting the appropriate architecture for the task at hand and serve as a basis for further investigation into the factors contributing to the superior performance of RNN and LSTM models in this context.